import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.naive_bayes import GaussianNB as gnb
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

train_df=pd.read_csv(r"G:\My Drive\E drive\CV Resources\GitHub-Kaggle\Disaster tweets\train.csv")
target_train=train_df.target

all_data=train_df
#.drop(['target'],axis=1)
all_data.head(10)

def show_wordcloud(data, title = None):
    wordcloud = WordCloud(
        background_color = 'white',
        max_words = 200,
        max_font_size = 40, 
        scale = 3,
        random_state = 42
    ).generate(str(data))

    fig = plt.figure(1, figsize = (10, 10))
    plt.axis('off')
    if title: 
        fig.suptitle(title, fontsize = 20)
        fig.subplots_adjust(top = 2.3)

    plt.imshow(wordcloud)
    plt.show()
    
# print wordcloud
show_wordcloud(all_data["text"])


#Removng stop words using NLTK
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop = stopwords.words('english')
# See https://www.mygreatlearning.com/blog/regular-expression-in-python/ for all regular expressions in Python
#Non-alphanumeric & non printable characters
all_data["text"]=all_data["text"].replace(r"[^a-zA-Z0-9]", ' ',regex=True)

#Email address
# "?" matches zero or 1 repetitions
# "x|y" matches either x or y
# "^" matches start of string
# "$" matches end of string
all_data["text"]=all_data["text"].replace(r"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|'(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*')@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])", ' ',regex=True)

#Numeric characsters
#\d metacharacters- denotes all numbers
# "+" any number before or after \d
all_data["text"]=all_data["text"].replace(r"[.+\d. +]", ' ',regex=True)

#URLs
all_data["text"]=all_data["text"].replace(r"/(?:(?:https?|ftp|file):\/\/|www\.|ftp\.)(?:\([-A-Z0-9+&@#\/%=~_|$?!:,.]*\)|[-A-Z0-9+&@#\/%=~_|$?!:,.])*(?:\([-A-Z0-9+&@#\/%=~_|$?!:,.]*\)|[A-Z0-9+&@#\/%=~_|$])/igm", ' ',regex=True)


#Converts all characters to lowercase
all_data["text"]=all_data["text"].str.lower()

#Removing stop words
all_data["text"] = all_data["text"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

#Multiple whitespaces to one
all_data["text"]=all_data["text"].str.strip()

all_data

#Dropping extra columns (Location & ID)
all_data=all_data.drop(['id','location'],axis=1)



#Dropping rows haiving NAN in any field
all_data=all_data.dropna()



#Decising encoding scheme for keyword
all_data['keyword'].value_counts().shape
#Viewing unique words
all_data['keyword'].unique()

#Eyeballing reveals similar pairs(electrocute,electrocuted),(catastrope,catastrophic)
#Cleaning keyword column
#Non-alphanumeric & non printable characters
all_data['keyword']=all_data['keyword'].replace(r"[^a-zA-Z0-9]", ' ',regex=True)

#Email address
# "?" matches zero or 1 repetitions
# "x|y" matches either x or y
# "^" matches start of string
# "$" matches end of string
all_data['keyword']=all_data['keyword'].replace(r"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|'(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*')@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])", ' ',regex=True)

#Numeric characsters
#\d metacharacters- denotes all numbers
# "+" any number before or after \d
all_data['keyword']=all_data['keyword'].replace(r"[.+\d. +]", ' ',regex=True)

#URLs
all_data['keyword']=all_data['keyword'].replace(r"/(?:(?:https?|ftp|file):\/\/|www\.|ftp\.)(?:\([-A-Z0-9+&@#\/%=~_|$?!:,.]*\)|[-A-Z0-9+&@#\/%=~_|$?!:,.])*(?:\([-A-Z0-9+&@#\/%=~_|$?!:,.]*\)|[A-Z0-9+&@#\/%=~_|$])/igm", ' ',regex=True)


#Converts all characters to lowercase
all_data['keyword']=all_data['keyword'].str.lower()

#Removing stop words
all_data['keyword'] = all_data['keyword'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

#Multiple whitespaces to one
all_data['keyword']=all_data['keyword'].str.strip()

#One-hot encoding introduces too many variables, using binary encoder for keyowrd column
from category_encoders import BinaryEncoder
be=BinaryEncoder(cols=['keyword'])
newcolumns=be.fit_transform(all_data['keyword'])
all_data=pd.concat([all_data,newcolumns],axis=1)
all_data


#Add the TF-IDF (Term Frequency â€” Inverse Document Frequency) values for every word and every document.
#and add TF-IDF columns

from sklearn.feature_extraction.text import TfidfVectorizer
#Selecting top 100 words from acc to TF-IDF values
tfidf = TfidfVectorizer(max_features=1000)
tfidf_result = tfidf.fit_transform(all_data["text"]).toarray()

#returns names of featurs selected, can be used to map back to original features
tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())
tfidf_df.columns = ["word_" + str(x) for x in tfidf_df.columns]
tfidf_df.index = all_data.index
all_data = pd.concat([all_data, tfidf_df], axis=1)


#Splitting in training and test
#Dropping text
all_data=all_data.drop(['keyword','text'],axis=1)
X_train, X_test, y_train, y_test = train_test_split(all_data.drop(['target'],axis=1), all_data['target'], test_size=0.30, random_state=42)


#SVM classifier

SVM = svm.SVC(kernel='rbf',gamma='scale',probability=True)
SVM.fit(X_train,y_train)
predictions_SVM = SVM.predict(X_test)

print('The accuracy of the SVM is',round(accuracy_score(predictions_SVM,y_test)*100,2))
sns.heatmap(confusion_matrix(y_test,predictions_SVM),annot=True,fmt='', cmap='Blues')
plt.title('Confusion_matrix', y=1.05, size=15)

#Naive Bayes 
gnb=GaussianNB()
predictions_NB=gnb.fit(X_train, y_train).predict(X_test)
print('The accuracy of the Naive Bayes is',round(accuracy_score(predictions_NB,y_test)*100,2))
sns.heatmap(confusion_matrix(y_test,predictions_NB),annot=True,fmt='3.0f')
plt.title('Confusion_matrix', y=1.05, size=15)

#Decision Tree
dtree=DecisionTreeClassifier.tree()
predictions_DT=dtree.fit(X_train, y_train).predict(X_test)
print('The accuracy of the Decision Tree is',round(accuracy_score(predictions_DT,y_test)*100,2))
sns.heatmap(confusion_matrix(y_test,predictions_DT),annot=True,fmt='3.0f')
plt.title('Confusion_matrix', y=1.05, size=15)
